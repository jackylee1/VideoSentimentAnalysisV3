{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time Emotion Detection on Video - Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/kerasv6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json, load_model\n",
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20, 20, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 18, 18, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 18, 18, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 338,466\n",
      "Trainable params: 337,762\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gender_model = load_model('./Trained Model/trained_GenderNet.hdf5')\n",
    "gender_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20, 20, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 18, 18, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 18, 18, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 339,111\n",
      "Trainable params: 338,407\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('./Trained Model/face_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"./Trained Model/face_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model = loaded_model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Explanation\n",
    "\n",
    "The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
    "\n",
    "The output is a 7 dim sofmax\n",
    "\n",
    "train.csv contains two columns, \"emotion\" and \"pixels\". The \"emotion\" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image. The \"pixels\" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. test.csv contains only the \"pixels\" column and your task is to predict the emotion column.\n",
    "\n",
    "The training set consists of 28,709 examples. The public test set used for the leaderboard consists of 3,589 examples. The final test set, which was used to determine the winner of the competition, consists of another 3,589 examples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(net_output):\n",
    "    '''EmotionNet output decoder.\n",
    "    \n",
    "    '''\n",
    "    if (np.argmax(net_output) == 0):\n",
    "        prediction = 'Angry'\n",
    "    elif (np.argmax(net_output) == 1):\n",
    "        prediction = 'Disgust'\n",
    "    elif (np.argmax(net_output) == 2):\n",
    "        prediction = 'Fear'\n",
    "    elif (np.argmax(net_output) == 3):\n",
    "        prediction = 'Happy'\n",
    "    elif (np.argmax(net_output) == 4):\n",
    "        prediction = 'Sad'\n",
    "    elif (np.argmax(net_output) == 5):\n",
    "        prediction = 'Surprise'\n",
    "    else:\n",
    "        prediction = 'Neutral'\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "\n",
    "def decode2(net_output, threshold = 0.3):\n",
    "    '''EmotionNet output decoder.\n",
    "    \n",
    "    '''\n",
    "    # Check if probability is higher than threshold\n",
    "    if np.max(net_output) >= threshold:\n",
    "        probability = str(np.max(net_output))\n",
    "        if (np.argmax(net_output) == 0):\n",
    "            prediction = 'Angry'\n",
    "        elif (np.argmax(net_output) == 1):\n",
    "            prediction = 'Disgust'\n",
    "        elif (np.argmax(net_output) == 2):\n",
    "            prediction = 'Fear'\n",
    "        elif (np.argmax(net_output) == 3):\n",
    "            prediction = 'Happy'\n",
    "        elif (np.argmax(net_output) == 4):\n",
    "            prediction = 'Sad'\n",
    "        elif (np.argmax(net_output) == 5):\n",
    "            prediction = 'Surprise'\n",
    "        else:\n",
    "            prediction = 'Neutral'\n",
    "    else:\n",
    "        # Do not label the box if you are not sure\n",
    "        prediction = ''\n",
    "        probability = ''\n",
    "    return prediction + ' ' + probability[:5]\n",
    "       \n",
    "       \n",
    "def pre_processing(color_image_rectangle):\n",
    "    '''Transform input image to EmotionNet input size.\n",
    "    \n",
    "    '''\n",
    "    # Convert image to B&W\n",
    "    gray_img = cv2.cvtColor(color_image_rectangle,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize image\n",
    "    gray_img = cv2.resize(gray_img,(48,48), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    # Reshape and add mini-batch dimension\n",
    "    gray_img = np.reshape(gray_img,(48,48,1))\n",
    "    gray_img = np.expand_dims(gray_img, axis=0)\n",
    "                      \n",
    "    return gray_img\n",
    "\n",
    "def gender_decoder(net_output, threshold=0.65):\n",
    "    '''GenderNet label decoder -----------------------\n",
    "\n",
    "    0: male\n",
    "    1: female\n",
    "    threshold: Probability minimum threshold to assign a label.\n",
    "    '''\n",
    "    if np.max(net_output) > threshold:\n",
    "        probability = str(np.max(net_output))\n",
    "        if np.argmax(net_output) == 0:\n",
    "            prediction = 'Male'\n",
    "        else:\n",
    "            prediction = 'Female'\n",
    "    else:\n",
    "        prediction = ''\n",
    "        probability = ''\n",
    "\n",
    "    return prediction + ' ' + probability[:5]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:4044: error: (-215) ssize.width > 0 && ssize.height > 0 in function resize\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-858e6b83db4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Resize frame of video to 1/4 size for faster face recognition processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msmall_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:4044: error: (-215) ssize.width > 0 && ssize.height > 0 in function resize\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load video\n",
    "vidcap = cv2.VideoCapture('./Videos/sad_affleck.mp4')\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    #ret, frame = video_capture.read()\n",
    "    success , frame = vidcap.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "    \n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]  # size (180, 320, 3)\n",
    "    \n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame,\n",
    "                                                        number_of_times_to_upsample=1)\n",
    "        #face_locations = face_recognition.face_locations(rgb_small_frame, \n",
    "        #                                                 model='CNN',\n",
    "        #                                                 number_of_times_to_upsample=1)\n",
    "        \n",
    "        face_emotions = []\n",
    "        for face in face_locations:\n",
    "            \n",
    "            # Get face coordinates\n",
    "            top, right, bottom, left = face\n",
    "\n",
    "            # Crop image \n",
    "            face_image = rgb_small_frame[top:bottom, left:right]\n",
    "            \n",
    "            # Pre-process image\n",
    "            gray_img = pre_processing(face_image)\n",
    "            \n",
    "            # Call model.predict() method\n",
    "            net_output = model.predict(gray_img)\n",
    "            net2_output = gender_model.predict(gray_img)\n",
    "\n",
    "            #print(net_output)\n",
    "            \n",
    "            # Decode output\n",
    "            prediction = decode2(net_output,0.30)\n",
    "            gender_prediction = gender_decoder(net2_output)\n",
    "            \n",
    "            prediction = prediction + '\\n' + gender_prediction\n",
    "            \n",
    "            # Add emotion to rectangle name\n",
    "            face_emotions.append(prediction)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), emotion in zip(face_locations, face_emotions):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "        \n",
    "        overlay = frame.copy()\n",
    "    \n",
    "        # Draw a box around the face\n",
    "#       cv2.rectangle(overlay, (left, top), (right, bottom), (0, 0, 255), 2) # Red\n",
    "#        cv2.rectangle(overlay, (left, top), (right, bottom), (139, 0, 0), 2) # Blue\n",
    "        cv2.rectangle(overlay, (left, top), (right, bottom), (180, 0, 0), 2) #Blue\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        y0, dy = 0, 30\n",
    "        for i, line in enumerate(emotion.split('\\n')):\n",
    "            y = y0 + i*dy\n",
    "            cv2.putText(overlay, line, (left + 6, bottom - 6 + y), font, 1.0, (255, 255, 255), 1)\n",
    "            #cv2.putText(overlay, line, (50, y ), cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        #cv2.rectangle(overlay, (left, bottom - 35), (right, bottom + 50), (0, 0, 255), cv2.FILLED)\n",
    "        #font = cv2.FONT_HERSHEY_DUPLEX\n",
    " \n",
    "        ############## REAL ONE\n",
    "        ##font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        ##cv2.putText(overlay, emotion, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "        ######################\n",
    "    \n",
    "        # cv2.putText(frame, emotion, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "        #cv2.circle(overlay, (133, 132), 12, (0, 255, 0), -1)\n",
    "        opacity = 0.5\n",
    "        cv2.addWeighted(overlay, opacity, frame, 1 - opacity, 0, frame)\n",
    " \n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "#video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kerasv6)",
   "language": "python",
   "name": "kerasv6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
